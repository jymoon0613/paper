### 2023.01.03
* compatible: 호환가능한, 양립할 수 있는
> <font size=1>The proposed two types of cross-attention are easy-to-implement and <span style="color: orange;">compatible</span> with self-attention learning.</font>

### 2022.12.31
* graft: 접목시키다
> <font size=1>To get the best of both worlds, in this paper, we propose a hybrid GZSL framework, <span style="color: orange;">grafting</span> an embedding model on top of a feature generation model.</font>

* suboptimal: 차선책, 최적이 아닌
> <font size=1>We conjecture that the original feature space, far from the semantic information and thus lack of discriminative ability, is <span style="color: orange;">suboptimal</span> for GZSL classification.</font>

### 2022.12.30
* inevitable: 불가피한
> <font size=1>However, since the embedding space is of high dimension and NN search is to be performed there, the hubness problem is <span style="color: orange;">inevitable</span>.</font>

### 2022.12.24
* plateau: 정체되다, 안정 상태에 도달하다
* canonical: 표준의
> <font size=1>Object detection performance, as measured on the <span style="color: orange;">canonical</span> PASCAL VOC dataset, has <span style="color: orange;">plateaued</span> in the last few years.</font>

### 2022.12.23
* degrade: 저하시키다
> <font size=1>Removing position embedding of our model does not <span style="color: orange;">degrade</span> the performance.</font>

### 2022.12.22
* feasible: 실행할 수 있는, 실현 가능한, 가능한
> <font size=1>To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is <span style="color: orange;">feasible</span> due to the use of very small (3×3) convolution filters in all layers.</font>

### 2022.12.21
* conjecture: 추측하다, 어림짐작하다
> <font size=1>We <span style="color: orange;">conjecture</span> that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error.</font>

* enrich: 풍부하게 하다
> <font size=1>Deep networks naturally integrate low/mid/highlevel features and classifiers in an end-to-end multilayer fashion, and the “levels” of features can be <span style="color: orange;">enriched</span> by the number of stacked layers (depth).</font>

* breakthrough: 돌파구, 획기적인 발전
> <font size=1>Deep convolutional neural networks have led to a series of <span style="color: orange;">breakthroughs</span> for image classification.</font>

### 2022.12.20
* alleviate: 완화하다
> <font size=1>The problem is <span style="color: orange;">alleviated</span> by InPS, which focuses on attended regions swapping, thus reducing interference from the background.</font>

### 2022.12.18
* shortcut: 지름길, 쉬운 방법
> <font size=1>Neural nets may exploit this <span style="color: orange;">shortcut</span> to solve the predictive task.</font>

* lag behind: 뒤처지다, 더디다
> <font size=1>But supervised pre-training is still dominant in computer vision, where unsupervised methods generally <span style="color: orange;">lag behind</span>.</font>

### 2022.12.16
* forego: 포기하다
> <font size=1>But to avoid using low-level features SSD <span style="color: orange;">foregoes</span> reusing already computed layers.</font>

* vastly: 엄청나게
> <font size=1>Recognizing objects at <span style="color: orange;">vastly</span> different scales is a fundamental challenge in computer vision.</font>

### 2022.12.15
* innate: 타고난
> <font size=1>Vision Transformer model is perfectly suited here with its <span style="color: orange;">innate</span> multi-head attention mechanism.</font>

### 2022.12.14
* aggravate: 악화시키다
> <font size=1>It leads to the problem of discriminative region diffusion (DRD) in WFGIR, which <span style="color: orange;">aggravates</span> the difficulty of discriminative region localization.</font>

### 2022.12.13
* In the following: 다음에, 이어서
> <font size=1><span style="color: orange;">In the following</span>, we mainly summarize and discuss those related works.</font>

* it is worth mentioning that: ~은 언급할 가치가 있다, 언급할 필요가 있다
> <font size=1>Additionally, <span style="color: orange;">it is worth mentioning that</span> one can easily embed API into any CNN backbones for fine-grained classification, and flexibly unload it for single-input test images without loss of generalization capacity.</font>

* progressive: 점진적인
> <font size=1>More specifically, API can effectively recognize two finegrained images, by a <span style="color: orange;">progressive</span> comparison procedure like human beings.</font>

### 2022.12.12
* notwithstanding: 그럼에도 불구하고
> <font size=1>Accuracy <span style="color: orange;">notwithstanding</span>, we would like for posenormalized representations to be human-interpretable, unlike prior black-box representations.</font>

### 2022.12.09
* resemblance: 닮음, 유사함
> <font size=1>However, fine-grained visual categorization (FGVC) poses a significant challenge mainly due to the close <span style="color: orange;">resemblance</span> between subcategories.</font>

### 2022.12.08
* exploit: (어떤 자원으로부터 이익을 얻기 위해 그 자원을) 사용하다
> <font size=1>ACNet uses a coarse-to-fine hierarchical feature learning process to <span style="color: orange;">exploit</span> the discriminative feature for classification.</font>

### 2022.12.07
* standard in the literatures: (관련된 연구) 문헌들의 표준
> <font size=1>All the above settings are <span style="color: orange;">standard in the literatures</span>.</font>

* ad-hoc manner: (계획이나 준비 없이) 즉석으로, 임시방편으로
> <font size=1>This differs significantly to prior art where parts are first detected, and later fused in an <span style="color: orange;">ad-hoc manner</span>.</font>

* consolidated: 통합된
> <font size=1>More specifically, we propose a <span style="color: orange;">consolidated</span> framework that accommodates part granularity learning and cross-granularity feature fusion simultaneously.</font>

* be attributed to: ~의 덕분이다, ~때문이다
> <font size=1>Success behind these models can <span style="color: orange;">be largely attributed to</span> being able to locate more discriminative local regions for downstream classification.</font>

* It follows that ~: 이렇게, 결과적으로, 이후에
> <font size=1><span style="color: orange;">It follows that</span> such locally discriminative features are collectively fused to perform final classification.</font>

### 2022.12.06
* intractable: 매우 다루기 힘든, 고질적인
> <font size=1>Apparently, <span style="color: orange;">intractable</span> problems can sometimes be solved when a different person looks at them.</font>

* delve into: 탐구하다
> <font size=1>The key insight lies with how we <span style="color: orange;">delve into</span> feature channels early on, as opposed to learning fine-grained part-level features on feature maps directly.</font>

* salient: 현저한, 두드러지는
> <font size=1>This design choice has a few <span style="color: orange;">salient</span> advantages over prior art.</font>

* cumbersome: 번거로운, 성가신
* error-prone: 오류가 발생하기 쉬운
> <font size=1>This is because expert human annotations can be <span style="color: orange;">cumbersome</span> to obtain and are often <span style="color: orange;">error-prone</span>.</font>

### 2022.12.05
* restrain: 제지하다, 억제하다
> <font size=1>To <span style="color: orange;">restrain</span> from the negative transfer between label predictions at different granularity, we first explicitly disentangle the decision space by constructing granularity-specific classification heads.</font>

* compelling evidence: 강력한 증거
> <font size=1>This provides <span style="color: orange;">compelling evidence</span> to the discovery we mentioned earlier.</font>

* granularity: 세분성
> <font size=1>Our first goal is to investigate the impact of transfer between label prediction tasks at different <span style="color: orange;">granularities</span>.</font>

* undergo: (어떤 것을) 경험하다, (어떠한 과정을) 거치다
> <font size=1>A major stream of FGVC works thus <span style="color: orange;">undergoes</span> two stages by first adopting a localisation sub-network to localise key visual cues and then a classification subnetwork to perform label prediction.</font>

* resort: (어려운 상황을 해결하기 위해, 문제를 해결하기 위해) ~을 사용하다
> <font size=1>To further encourage the disentanglement, we then <span style="color: orange;">resort</span> to the clever use of gradients to reflect our second observation.</font>

* stride: 걸음, 보폭
* surge: 급등하다, 급등
* tackle: 다루다
> <font size=1>Great <span style="color: orange;">strides</span> have been made over the years, starting with the conventional part-based models, to the recent <span style="color: orange;">surge</span> of deep models that either explicitly or implicitly <span style="color: orange;">tackle</span> part learning with or without strong supervision.</font>

* overlook: 간과하다
> <font size=1>It brought out a critical question that was largely <span style="color: orange;">overlooked</span> back then – that can machines match up to humans on recognising objects at fine-grained level.</font>

* disentangle: (얽힌 것을) 풀다
> <font size=1>We leverage level-specific classification heads to <span style="color: orange;">disentangle</span> coarse-level features with fine-grained ones.</font>

* exacerbate: 악화시키다
> <font size=1>Coarse-level label prediction <span style="color: orange;">exacerbates</span> fine-grained feature learning, yet fine-level feature betters the learning of coarse-level classifier.</font>

* envisage: 상상하다, 생각하다
> <font size=1>For that, we <span style="color: orange;">re-envisage</span> the traditional setting of FGVC, from single-label clas- sification, to that of top-down traversal of a pre-defined coarse-to-fine label hierarchy.</font>

### 2022.12.02
* regime: 영역, 특정한 상황 또는 규칙
> <font size=1>It outperforms fully supervised ImageNet models and increases robustness in small data <span style="color: orange;">regimes</span>, reducing annotation cost across multiple medical imaging applications.</font>

* synergistic effects: 시너지 효과
> <font size=1>Existing efforts, however, omit their <span style="color: orange;">synergistic effects</span> on each other in a ternary setup.</font>

* foster: 기르다, 육성하다
> <font size=1>Can discriminative, restorative, and adversarial learning be seamlessly integrated into a single framework to <span style="color: orange;">foster</span> collaborative learning for deep semantic representation.</font>

* glean: 줍다, 수집하다
> <font size=1>We have designed a novel self-supervised learning framework, called DiRA, by uniting discriminative learning, restorative learning, and adversarial learning in a unified manner to <span style="color: orange;">glean</span> complementary visual information from unlabeled medical im ages.</font>

* leverage: (이점을 최대화하기 위해 어떤 것을) 활용하다, 사용하다 
> <font size=1>Our restorative learning branch aims to enhance discrimination learning by <span style="color: orange;">leveraging</span> fine-grained visual information. </font>

### 2022.12.01
* deformation: 변형
> <font size=1>Good representation in fine-grained classification should be invariant to the <span style="color: orange;">deformations</span> of object parts and the changes of viewing angles.</font>

* devote to: ~에 전념하다
> <font size=1>A large number of deep approaches were <span style="color: orange;">devoted to</span> the former challenge of capturing finegrained details in local regions.</font>

* besides: 게다가, 이외에도, 뿐만 아니라
> <font size=1><span style="color: orange;">Besides</span>, we also measure the compactness of image representations.</font>

### 2022.11.30
* subordinate: 종속적인, 아래의, 하위의
> <font size=1>The problem of classifying fine-grained objects is made more challenging due to the inherently subtle shape difference across the <span style="color: orange;">subordinate</span> categories.</font>

* non-trivial: 사소하지 않은, 중요한, 어려운
> <font size=1>Learning a discriminative representation for fine-grained objects remains <span style="color: orange;">non-trivial</span> in the context of deep learning.</font>

### 2022.11.28
* counterpart: 상대방, 대응물
> <font size=1>Each model can thus benefit from its <span style="color: orange;">counterpart</span> by utilizing cross-model predictions as supervision.</font>

* to this end: 이를 위해
> <font size=1>We want to save the building. <span style="color: orange;">To this end</span>, we have hired someone to assess its current state.</font>

* to the best of our knowledge: 우리가 아는 한
> <font size=1><span style="color: orange;">To the best of our knowledge</span>, that design will not work.</font>

* in particular: 특히
> <font size=1><span style="color: orange;">In particular</span>, it better captures temporal dynamics in recognizing actions.</font>

* empirical analysis: 실증적 분석, 실험을 통해 증명하는 분석
> <font size=1>We also conduct a comprehensive <span style="color: orange;">empirical analysis</span> to study how the cross-model supervision helps improve performance.</font>

* incentivize: 장려하다, 어떤 일을 하도록 유도하다
> <font size=1>This symmetric design <span style="color: orange;">incentivizes</span> the two models to learn complementary representations.</font>

*  auxiliary: 보조, 보조적인 (assistant)
> <font size=1>The primary backbone is supplemented by a lightweight <span style="color: orange;">auxiliary</span> network with a different structure and fewer channels than the backbone.</font>

* among: ~중에
> <font size=1><span style="color: orange;">Among</span> two strategies, task-dynamic feature alignment methods are being spotlighted.</font>
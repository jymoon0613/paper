### 2023.09.07
* impair: 손상시키다
> <font size=1>Nevertheless, shallow features have less semantic information, which may ***impair*** the performance of small object detection.</font>  

### 2023.09.04
* undermine: 약화시키다, 훼손하다
> <font size=1>They believe that aligning the source domain and the target domain globally leads to negative transfer of information and ***undermines*** the performance of the model in the originally well-aligned regions.</font>  

### 2023.08.15
* prosperity: 번영
> <font size=1>The ***prosperity*** of the Convolutional Neural Networks (CNN) comes from its rich representation capacity and powerful generalization ability in image recognition, which is proved in challenging ImageNet classification task.</font>  

### 2023.08.09
* cascade: 단계적인, 위에서 아래로 내려오는, (일방적인) 흐름/전달
> <font size=1>This approach can be seen as a two-stage ***cascade***: First, selection of a reduced set of promising and class-independent hypotheses and second, a class-specific classification of each hypothesis.</font>  

### 2023.08.04
* disparate: 다른, 이질적인
> <font size=1>For instance, many ***disparate*** object categories might share high-level structures (such as the limbs of animals and robots) and detecting such structures might hint towards the presence of objects.</font>  

### 2023.08.01
* cluttered: 어수선한, 어지러운
> <font size=1>We propose a new task, UOAIS, to detect categoryagnostic visible masks, amodal masks, and occlusion of arbitrary object instances in a ***cluttered*** environment.</font>  

### 2023.07.27
* deter: 저지하다, 억제하다
> <font size=1>The large model sizes and expensive computation costs ***deter*** their deployment in many real-world applications such as robotics and self-driving cars where model size and latency are highly constrained.</font>  

### 2023.07.26
* compromise: 타협하다, 절충하다
> <font size=1>Increasing dilation rates enlarges the effective receptive field by emphasizing large objects, thus ***compromising*** the performance of small objects.</font>  
> <font size=1>As a result, it could achieve significant improvement over the single-scale baseline without any ***compromise*** of inference speed.</font>

### 2023.07.19
* commence: 시작하다
> <font size=1>The process typically ***commences*** with a training set consisting of all object examples and a small, random set of background examples.</font>

* odd: 이상한, 기대와는 다른
> <font size=1>It may seem ***odd*** then that the current state-of-the-art object detectors, embodied by Fast R-CNN and its descendants, do not use bootstrapping.</font>

### 2023.04.28
* decouple: 분리하다
> <font size=1>Recent studies have shown that directly training the rebalancing strategy would degrade the performance of representation extraction, so some multistage training methods ***decouple*** the training of representation learning and classifier for long-tail recognition.</font>

### 2023.04.11
* as a remedy: 해결책으로
> <font size=1>***As a remedy***, we introduce a distributional prior to regularize halting scores such that tokens are expected to exit at a target depth on average, however, we still allow per-image variations.</font>

### 2023.04.07
* agnostic: 세부 정보를 알지 않거나 요구하지 않고 작동하는
> <font size=1>On the other hand, the sparse attention adopted in PVT or Swin Transformer is data ***agnostic*** and may limit the ability to model long range relations.</font>

### 2023.04.04
* on-the-fly: 즉시, 즉석에서, 그때그때
> <font size=1>Optimized jointly with a transformer backbone in an end-to-end manner, a light-weight decision network is attached to the backbone to produce decisions ***on-the-fly***.</font>

### 2023.03.24
* in short: 간단히 말해서, 요컨대
> <font size=1>***In short***, CNNs are inherently limited to model large, unknown transformations.</font>

### 2023.03.14
* impede: 방해하다
> <font size=1>To achieve this result, we identify class imbalance during training as the main obstacle ***impeding*** one-stage detector from achieving state-of-the-art accuracy and propose a new loss function that eliminates this barrier.</font>

### 2023.03.10
* coherent: 응집된, 밀착된, 일관된
> <font size=1>Our key observation is that features from a convolutional network can be used to group pixels into a set of visually ***coherent*** regions, from which a subset of discriminative segments can be selected for recognition.</font>

### 2023.03.08
* versatile: 변하기 쉬운, 다목적, 다재다능한
> <font size=1>By incorporating the pyramid structure from CNNs, we present the Pyramid Vision Transformer (PVT), which can be used as a ***versatile*** backbone for many computer vision tasks, broadening the scope and impact of ViT.</font>

### 2023.03.02
* discrepancy: (같아야 하는 두 것 사이의) 불일치
> <font size=1>There is a growing ***discrepancy*** in computer vision between large-scale models that achieve state-of-the-art performance and models that are affordable in practical applications.</font>

### 2023.02.14
* intriguingly: 흥미롭게도, 신기하게도
> <font size=1>***Intriguingly***, Barlow Twins strongly benefits from the use of very high-dimensional embeddings.</font>

### 2023.02.10
* in line with: ~과 일치하는, ~에 맞춰
> <font size=1>This is ***in line with*** prior work, who also observed improved performance with such an auxiliary objective.</font>

### 2023.02.07
* degenerated: 퇴화된, 변질된, 뒤떨어지는
> <font size=1>However, directly aligning the embeddings usually causes ***degenerated*** solutions.</font>

### 2023.01.16
* counter-intuitive: 직관적이지 않은, 일반적인 생각과는 다른
> <font size=1>A possibly ***counter-intuitive*** effect of this dense connectivity pattern is that it requires fewer parameters than traditional convolutional networks, as there is no need to relearn redundant feature-maps.</font>

### 2023.01.11
* without bells and whistles: (멋으로 덧붙이는 거추장스러운) 부가적인 모듈 없이
> <font size=1>***Without bells and whistles***, Mask R-CNN surpasses all previous state-of-the-art single-model results on the COCO instance segmentation task, including the heavilyengineered entries from the 2016 competition winner.</font>

### 2023.01.03
* compatible: 호환가능한, 양립할 수 있는
> <font size=1>The proposed two types of cross-attention are easy-to-implement and ***compatible*** with self-attention learning.</font>

### 2022.12.31
* graft: 접목시키다
> <font size=1>To get the best of both worlds, in this paper, we propose a hybrid GZSL framework, ***grafting*** an embedding model on top of a feature generation model.</font>

* suboptimal: 차선책, 최적이 아닌
> <font size=1>We conjecture that the original feature space, far from the semantic information and thus lack of discriminative ability, is ***suboptimal*** for GZSL classification.</font>

### 2022.12.30
* inevitable: 불가피한
> <font size=1>However, since the embedding space is of high dimension and NN search is to be performed there, the hubness problem is ***inevitable***.</font>

### 2022.12.24
* plateau: 정체되다, 안정 상태에 도달하다
* canonical: 표준의
> <font size=1>Object detection performance, as measured on the ***canonical*** PASCAL VOC dataset, has ***plateaued*** in the last few years.</font>

### 2022.12.23
* degrade: 저하시키다
> <font size=1>Removing position embedding of our model does not ***degrade*** the performance.</font>

### 2022.12.22
* feasible: 실행할 수 있는, 실현 가능한, 가능한
> <font size=1>To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is ***feasible*** due to the use of very small (3×3) convolution filters in all layers.</font>

### 2022.12.21
* conjecture: 추측하다, 어림짐작하다
> <font size=1>We ***conjecture*** that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error.</font>

* enrich: 풍부하게 하다
> <font size=1>Deep networks naturally integrate low/mid/highlevel features and classifiers in an end-to-end multilayer fashion, and the “levels” of features can be ***enriched*** by the number of stacked layers (depth).</font>

* breakthrough: 돌파구, 획기적인 발전
> <font size=1>Deep convolutional neural networks have led to a series of ***breakthroughs*** for image classification.</font>

### 2022.12.20
* alleviate: 완화하다
> <font size=1>The problem is ***alleviated*** by InPS, which focuses on attended regions swapping, thus reducing interference from the background.</font>

### 2022.12.18
* shortcut: 지름길, 쉬운 방법
> <font size=1>Neural nets may exploit this ***shortcut*** to solve the predictive task.</font>

* lag behind: 뒤처지다, 더디다
> <font size=1>But supervised pre-training is still dominant in computer vision, where unsupervised methods generally ***lag behind***.</font>

### 2022.12.16
* forego: 포기하다
> <font size=1>But to avoid using low-level features SSD ***foregoes*** reusing already computed layers.</font>

* vastly: 엄청나게
> <font size=1>Recognizing objects at ***vastly*** different scales is a fundamental challenge in computer vision.</font>

### 2022.12.15
* innate: 타고난
> <font size=1>Vision Transformer model is perfectly suited here with its ***innate*** multi-head attention mechanism.</font>

### 2022.12.14
* aggravate: 악화시키다
> <font size=1>It leads to the problem of discriminative region diffusion (DRD) in WFGIR, which ***aggravates*** the difficulty of discriminative region localization.</font>

### 2022.12.13
* In the following: 다음에, 이어서
> <font size=1>***In the following***, we mainly summarize and discuss those related works.</font>

* it is worth mentioning that: ~은 언급할 가치가 있다, 언급할 필요가 있다
> <font size=1>Additionally, ***it is worth mentioning that*** one can easily embed API into any CNN backbones for fine-grained classification, and flexibly unload it for single-input test images without loss of generalization capacity.</font>

* progressive: 점진적인
> <font size=1>More specifically, API can effectively recognize two finegrained images, by a ***progressive*** comparison procedure like human beings.</font>

### 2022.12.12
* notwithstanding: 그럼에도 불구하고
> <font size=1>Accuracy ***notwithstanding***, we would like for posenormalized representations to be human-interpretable, unlike prior black-box representations.</font>

### 2022.12.09
* resemblance: 닮음, 유사함
> <font size=1>However, fine-grained visual categorization (FGVC) poses a significant challenge mainly due to the close ***resemblance*** between subcategories.</font>

### 2022.12.08
* exploit: (어떤 자원으로부터 이익을 얻기 위해 그 자원을) 사용하다
> <font size=1>ACNet uses a coarse-to-fine hierarchical feature learning process to ***exploit*** the discriminative feature for classification.</font>

### 2022.12.07
* standard in the literatures: (관련된 연구) 문헌들의 표준
> <font size=1>All the above settings are ***standard in the literatures***.</font>

* ad-hoc manner: (계획이나 준비 없이) 즉석으로, 임시방편으로
> <font size=1>This differs significantly to prior art where parts are first detected, and later fused in an ***ad-hoc manner***.</font>

* consolidated: 통합된
> <font size=1>More specifically, we propose a ***consolidated*** framework that accommodates part granularity learning and cross-granularity feature fusion simultaneously.</font>

* be attributed to: ~의 덕분이다, ~때문이다
> <font size=1>Success behind these models can ***be largely attributed to*** being able to locate more discriminative local regions for downstream classification.</font>

* It follows that ~: 이렇게, 결과적으로, 이후에
> <font size=1>***It follows that*** such locally discriminative features are collectively fused to perform final classification.</font>

### 2022.12.06
* intractable: 매우 다루기 힘든, 고질적인
> <font size=1>Apparently, ***intractable*** problems can sometimes be solved when a different person looks at them.</font>

* delve into: 탐구하다
> <font size=1>The key insight lies with how we ***delve into*** feature channels early on, as opposed to learning fine-grained part-level features on feature maps directly.</font>

* salient: 현저한, 두드러지는
> <font size=1>This design choice has a few ***salient*** advantages over prior art.</font>

* cumbersome: 번거로운, 성가신
* error-prone: 오류가 발생하기 쉬운
> <font size=1>This is because expert human annotations can be ***cumbersome*** to obtain and are often ***error-prone***.</font>

### 2022.12.05
* restrain: 제지하다, 억제하다
> <font size=1>To ***restrain*** from the negative transfer between label predictions at different granularity, we first explicitly disentangle the decision space by constructing granularity-specific classification heads.</font>

* compelling evidence: 강력한 증거
> <font size=1>This provides ***compelling evidence*** to the discovery we mentioned earlier.</font>

* granularity: 세분성
> <font size=1>Our first goal is to investigate the impact of transfer between label prediction tasks at different ***granularities***.</font>

* undergo: (어떤 것을) 경험하다, (어떠한 과정을) 거치다
> <font size=1>A major stream of FGVC works thus ***undergoes*** two stages by first adopting a localisation sub-network to localise key visual cues and then a classification subnetwork to perform label prediction.</font>

* resort: (어려운 상황을 해결하기 위해, 문제를 해결하기 위해) ~을 사용하다
> <font size=1>To further encourage the disentanglement, we then ***resort*** to the clever use of gradients to reflect our second observation.</font>

* stride: 걸음, 보폭
* surge: 급등하다, 급등
* tackle: 다루다
> <font size=1>Great ***strides*** have been made over the years, starting with the conventional part-based models, to the recent ***surge*** of deep models that either explicitly or implicitly ***tackle*** part learning with or without strong supervision.</font>

* overlook: 간과하다
> <font size=1>It brought out a critical question that was largely ***overlooked*** back then – that can machines match up to humans on recognising objects at fine-grained level.</font>

* disentangle: (얽힌 것을) 풀다
> <font size=1>We leverage level-specific classification heads to ***disentangle*** coarse-level features with fine-grained ones.</font>

* exacerbate: 악화시키다
> <font size=1>Coarse-level label prediction ***exacerbates*** fine-grained feature learning, yet fine-level feature betters the learning of coarse-level classifier.</font>

* envisage: 상상하다, 생각하다
> <font size=1>For that, we ***re-envisage*** the traditional setting of FGVC, from single-label classification, to that of top-down traversal of a pre-defined coarse-to-fine label hierarchy.</font>

### 2022.12.02
* regime: 영역, 특정한 상황 또는 규칙
> <font size=1>It outperforms fully supervised ImageNet models and increases robustness in small data ***regimes***, reducing annotation cost across multiple medical imaging applications.</font>

* synergistic effects: 시너지 효과
> <font size=1>Existing efforts, however, omit their ***synergistic effects*** on each other in a ternary setup.</font>

* foster: 기르다, 육성하다
> <font size=1>Can discriminative, restorative, and adversarial learning be seamlessly integrated into a single framework to ***foster*** collaborative learning for deep semantic representation.</font>

* glean: 줍다, 수집하다
> <font size=1>We have designed a novel self-supervised learning framework, called DiRA, by uniting discriminative learning, restorative learning, and adversarial learning in a unified manner to ***glean*** complementary visual information from unlabeled medical im ages.</font>

* leverage: (이점을 최대화하기 위해 어떤 것을) 활용하다, 사용하다 
> <font size=1>Our restorative learning branch aims to enhance discrimination learning by ***leveraging*** fine-grained visual information. </font>

### 2022.12.01
* deformation: 변형
> <font size=1>Good representation in fine-grained classification should be invariant to the ***deformations*** of object parts and the changes of viewing angles.</font>

* devote to: ~에 전념하다
> <font size=1>A large number of deep approaches were ***devoted to*** the former challenge of capturing finegrained details in local regions.</font>

* besides: 게다가, 이외에도, 뿐만 아니라
> <font size=1>***Besides***, we also measure the compactness of image representations.</font>

### 2022.11.30
* subordinate: 종속적인, 아래의, 하위의
> <font size=1>The problem of classifying fine-grained objects is made more challenging due to the inherently subtle shape difference across the ***subordinate*** categories.</font>

* non-trivial: 사소하지 않은, 중요한, 어려운
> <font size=1>Learning a discriminative representation for fine-grained objects remains ***non-trivial*** in the context of deep learning.</font>

### 2022.11.28
* counterpart: 상대방, 대응물
> <font size=1>Each model can thus benefit from its ***counterpart*** by utilizing cross-model predictions as supervision.</font>

* to this end: 이를 위해
> <font size=1>We want to save the building. ***To this end***, we have hired someone to assess its current state.</font>

* to the best of our knowledge: 우리가 아는 한
> <font size=1>***To the best of our knowledge***, that design will not work.</font>

* in particular: 특히
> <font size=1>***In particular***, it better captures temporal dynamics in recognizing actions.</font>

* empirical analysis: 실증적 분석, 실험을 통해 증명하는 분석
> <font size=1>We also conduct a comprehensive ***empirical analysis*** to study how the cross-model supervision helps improve performance.</font>

* incentivize: 장려하다, 어떤 일을 하도록 유도하다
> <font size=1>This symmetric design ***incentivizes*** the two models to learn complementary representations.</font>

*  auxiliary: 보조, 보조적인 (assistant)
> <font size=1>The primary backbone is supplemented by a lightweight ***auxiliary*** network with a different structure and fewer channels than the backbone.</font>

* among: ~중에
> <font size=1>***Among*** two strategies, task-dynamic feature alignment methods are being spotlighted.</font>

# Learning Transferable Visual Models From Natural Language Supervision (ICML, 2022)

[논문링크](https://proceedings.mlr.press/v139/radford21a)

<p align="center">
    <img width="800" alt='fig1' src="./img/05_20_01.png?raw=true"></br>
    <em><font size=2>Overall Architecture</font></em>
</p>

## 연구목적
- 본 논문에서는 image classifiers를 large-scale language supervision으로 학습시키고자 함 (contrastive language-image pre-training, CLIP)
- Language supervision으로 학습하는 것은 classic한 task-specific labels를 필요로 하지 않으므로 모델 scale에 유리함
- 또한, language supervision으로부터의 학습은 모델이 단순한 image representation이 아닌 image representation과 language의 연결 관계를 학습하게 하여 훨씬 flexible한 representation learning이 가능함

## 접근법
- 인터넷에 공개된 소스를 통해 4억 개의 image-text pairs로 구성된 데이터셋을 수집
> - 각 이미지는 영어 title 혹은 description이 같이 주어짐
- Image encoder와 text encoder 간의 contrastive learning으로 image-text pairs에 대한 pre-training 수행
> - 하나의 batch 내에서 $N$ 개의 올바른 image-text pairs 간의 embeddings의 cosine similarity가 최대화되도록 하며, $N^2-N$ 개의 incorrect pairs의 embeddings 간에는 최소화되도록 함

## 실험결과
- CLIP으로 학습된 모델의 zero-shot performance를 평가함
> - 'Image classification에서 unseen object categories에 대한 generalization'을 다루는 computer vision의 일반적인 zero-shot learning과 달리, 본 논문에서는 훨씬 광범위한 의미로 'Unseen datasets에 대한 generalization'을 다룸
- 주어진 dataset의 모든 arget classes를 text input으로 사용하여 embeddings를 추출하고, 주어진 image embedding과의 cosine similarity를 계산한 뒤, cosine similarity가 가장 큰 class로 예측 수행
- ImageNet에서 기존 SOTA 모델들의 성능을 능가했으며 연산도 훨씬 효율적임
- 또한, CLIP은 distribution shift에도 훨씬 robust했음

## 의견
- /
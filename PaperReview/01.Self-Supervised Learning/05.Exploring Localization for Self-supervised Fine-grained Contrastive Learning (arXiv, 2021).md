# Exploring Localization for Self-supervised Fine-grained Contrastive Learning (arXiv, 2021)

[논문 링크](https://arxiv.org/abs/2106.15788)

<p align="center">
    <img width="600" alt='fig1' src="./img/01_05_01.png?raw=true"></br>
    <em><font size=2>Overall Architecture</font></em>
</p>

## 연구목적
- 이전의 Pre-text Learning 방식들은 같은 이미지에 적용된 변환을 예측하여 불변적인 피처들을 모델이 학습하도록 하는 것이 목적 
- 최근에는 Contrastive Learning 방법이 많이 연구되고 있음 
- 최근의 Contrastive Learning 방식이 Supervised Learning의 성능을 앞지르기까지 하지만, 이는 오직 General Purpose Datasets (Coarse-grained) 한정임 
- 즉, Contrastive Learning은 추가 정보가 풍부한 배경 + 크고 구별이 쉬운 Object가 있는 일반적인 이미지 데이터셋에서는 효과적이지만, 배경이 주는 추가 정보가 거의 없고 Object간 구별이 쉽지 않은 Fine-grained 이미지 데이터셋에서는 효과가 거의 없음 
- 이는 Contrastive Learning이 Low-level Texture 정보는 잘 학습하도록 하지만, Foreground Object에 대한 Localization Ability는 매우 부족하기 때문임 
- 이러한 Localization 능력 부족은 Augmentation 과정에서 주로 채택되는 Random Resized Crop 때문인 것으로 보이는데, Random Resized Crop은 입력 이미지의 무작위 위치를 Crop하고 Resize하기 때문에 일반적인 이미지 데이터셋에서는 Object와 관련된 배경이나 주변 정보를 같이 학습할 수 있도록 해주지만 (e.g. P(새 | 나무) > P(자동차 | 나무)), Fine-grained 데이터셋에서는 이러한 배경이나 주변 정보가 필요가 없으며, 따라서 아무런 효과가 없고 오히려 Localization 능력을 낮출 뿐임 
- 이러한 Fine-grained 이미지 데이터셋에서의 Contrastive Learning이 갖는 문제점을 해결하고자, 본 연구에서는 Cross-View Saliency Alignment (CVSA)라는 사전 훈련 프레임워크를 제안하여 Augmented된 이미지에서 Fine-grained Semantic Features를 정렬함으로써 Fine-grained 이미지 데이터셋에서도 잘 작동하도록 Contrastive Learning을 개선하고자 함 
- 본 논문의 Contribution은 1) 현재의 여러 Self-supervised Learning 방법과 Supervised Learning 방법을 Fine-grained 데이터셋에서 비교하여 분석하고 그 한계를 도출함, 2) Saliency Swap이라는 Augmentation 기술을 사용하여 Fine-grained 이미지 데이터셋에 적용할 수 있는 Self-supervised Learning 방법을 제안함, 3) 여러 FGVC Task에서 성능 증가가 있음을 증명 

## 접근법
### (1) Fine-grained Pre-training Essentials 
- 현재의 Contrastive Learning이 FGVC에서 잘 작동하지 못하는 이유를 설명 
- FGVC는 우선 입력 이미지에서 대상이 되는 Object를 찾고, 해당 Object로부터 클래스 분류를 하는 문제로 나누어 생각해볼 수 있음 
- 따라서 모델의 SSL 훈련 과정도 두 단계로 나누어 생각해볼 수 있음 
- 우선 모델은 BYOL, MoCo와 같은 일반적인 Contrastive Learning을 통해 대규모 데이터셋에서 Discriminative한 피처를 추출할 수 있도록 학습시킴 
- 다음으로 모델은 제안하는 CVSA를 통해 Localization 능력을 학습하며 Background 정보가 클래스 레이블 결정에 영향을 주지 못하게 함 
### (2) Saliency Swap 
- Saliency Swap의 목적은 이미지 전반에 걸쳐 Foreground Object의 의미론적 일관성을 학습하기 위해 배경에 변화를 주면서 유니크한 특징 정보를 최대한 활용하는 것 
- Random Resized Crop과 달리 제안하는 Saliency Swap은 각 패치가 최소한 Object의 일부를 포함하도록 강제하므로 모델이 순수한 배경 정보를 통해 관련 없는 피처를 학습하는 것을 방지함 
- 또한, 소스 이미지의 중요 피처 영역과 다른 이미지의 중요 피처 영역을 교환하여 모호성을 방지함 
- 즉, 모델은 임의의 배경에 집중하는 것이 아니라 공통적인 Background에서 객체 지향적인 의미론적 특징을 추출해야 함 
- 소스 이미지에서 중요 피처 영역을 추출하기 위해 모든 픽셀 위치에서 Saliency Score를 계산하고, 임의의 크기의 Bounding Box를 바탕으로 해당 Bounding Box 내의 평균 Saliency Score가 가장 큰 구역을 중요 피처 영역으로 추출하여 Crop함 
- 이후 추출된 이미지 패치를 다른 이미지의 배경과 합침 (배경 추출 대상 이미지의 중요 피처 영역을 추출된 이미지 패치로 대체) 
### (3) Cross-view Saliency Alignment 
- Saliency Swap의 목적은 이미지 전반에 걸쳐 Foreground Object의 의미론적 일관성을 학습하기 위해 배경에 변화를 주면서 유니크한 특징 정보를 최대한 활용하는 것 
- Saliency Swap과 함께 여러 Augmentation이 적용된 두 이미지를 Backbone Network (e.g. ResNet50)로 처리하여 3차원 Feature Map을 추출함 
- SimCLR에서 사용한 두 개의 MLP로 Projector를 구성하고 피처 벡터를 추출함 
- 또한, 한 이미지에는 Convolution Projector를 사용하여 피처를 추출 
- Convolution Predictor와 MLP Predictor를 사용하여 피처 벡터를 추출함 
- Output은 3개, 차원은 동일, 두 이미지에 각각 적용되므로 최종 아웃풋은 3 x 2 = 6개 
- Convolution Predictor는 Saliency Alignment를 위해 사용됨 
- 이미지 Q에 Convolution Projector 및 Convolution Predictor 처리까지 완료한 결과와 이미지 K에 Convolution Projector 처리만 완료한 결과의 Transpose를 곱해 Cross View Attention Map을 생성함 (Q에 대한 K의 Attention Map, 반대로도 똑같이 구함) 
- 이러한 Attention Map을 Saliency Swap에 사용한 Saliency Mask와 비교하여 모델이 Object에 대한 Localization 능력을 학습할 수 있도록 함 
- 또한 Convolution이 아닌 MLP로 추출한 두 벡터 간의 유사도를 코사인 유사도로 평가하고, 최종적인 Loss Function은 Contrastive Loss + Alignment Loss가 됨 

## 실험결과
- Stage 1은 BYOL을 사용하여 학습함 
- 오직 Random Resized Crop을 Saliency Swap으로 변경하여 학습함 
- Saliency 영역은 Supervised하게 학습된 Saliency Detector를 사용함 
- 여러 SSL 방법을 FGVC 데이터셋에 적용한 결과와 비교함 
- 제안된 방법이 가장 좋았음 

## 의견
- 굳이 SOTA 성능 개선 X 
- SSL이 필요한 이유를 잘 정의하고
- SSL을 잘 설계하여 
- 다른 SSL을 적용했을 때보다 성능이 높다는 것을 보여주면 될 듯 

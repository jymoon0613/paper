# Pyramid With Super Resolution for In-the-Wild Facial Expression Recognition (IEEE Access, 2020)

[논문 링크](https://ieeexplore.ieee.org/abstract/document/9143068)

<p align="center">
    <img width="600" alt='fig1' src="./img/09_10_01.png?raw=true"></br>
    <em><font size=2>Overall Architecture</font></em>
</p>

## 연구목적
- 선행 연구에서는 60%-80%의 커뮤니케이션이 비언어적 방식으로 이루어지고 있으며, 이러한 비언어적 의사 소통에서 사용되는 대표적인 시그널은 표정임 
- FER이란 입력 얼굴 이미지에 대해 대응하는 표정 클래스를 예측하는 것을 의미함 
- 최근의 In-the-lab 데이터셋에 대한 좋은 성능에도 불구하고, 좀 더 일반화될 수 있는 In-the-wild 데이터셋에서의 성능 개선이 필요함 (e.g. RAF-DB, AffectNet, FER+) 
- 현재의 In-the-wild 데이터셋은 이미지의 크기가 일정하지 않다는 문제가 있으며, FER 모델들은 주로 Interpolation 기법을 사용한 Resize 방식을 전처리로 사용함 
- 이러한 이미지 사이즈 문제는 입력 이미지의 사이즈에 민감한 CNN의 특성과도 관련이 있음 
- 최근에는 Super Resolution (SR) 기법이 Image Resizing 측면에서 딥러닝 모델의 한 전처리 과정으로 활발히 활용되고 있음 
- 따라서, 본 연구에서는 Pyramid with Super Resolution (PSR) Network를 제안하여 In-the-wild 데이터셋의 이미지 크기 문제를 해결함과 동시에 Pyramid 구조를 통해 모델이 입력 이미지의 다양한 스케일 정보를 획득함으로써 성능을 개선하도록 함 
- 또한 이에 적합한 Loss Function도 제안함 

## 접근법
### (1) Overview 
- PSR Network는 총 6개의 Block으로 구성됨 
- Spatial Transformer Network (STN), Scaling, Low-level Feature Extractor, High-level Feature Extractor, Fully Connected, Final Concatenation Block 
### (2) Spatial Transformer Network (STN) Block 
- 모델에 뛰어난 Spatial Invariance 능력을 주기 위해 STN을 적용 
- 이미지의 특정 부분을 자르고 변환하여 해당 부분만 트레이닝 시킴 (입력 이미지를 정렬, Align the input) 
- 즉, 다양하게 표현되어 있는 In-the-wild 데이터셋의 이미지에서 얼굴에 해당하는 부분만 추출하는 역할 
### (3) Scaling Block 
- 모델이 입력 이미지를 다양한 스케일에서 볼 수 있도록 하는 역할 
- Image Upscale을 위해 Super Resolution이 사용됨 
- 원본 이미지를 사전에 정의된 스케일로 Downsampling한 뒤, Super Resolution으로 차례대로 Upsampling함 
- 여러 네트워크가 같은 이미지를 동시에 처리하지만, 각 네트워크는 서로 다른 스케일의 이미지를 처리 
- 가장 마지막의 네트워크는 가장 큰 스케일 (= 원본 이미지)를 처리 
### (4) Low and High-level Feature Extractor 
- VGG16을 Backbone 구조로 채택 
- VGG16을 두 부분으로 나누어 각각 Low-level Feature Extractor와 High-level Feature Extractor로 사용함 
- Low-level 피처는 서로 다른 이미지 크기 때문에 Branch 간 공유가 불가능 
- High-level 피처는 서로 다른 이미지 크기에 상관없이 공유가 가능하므로, 모든 Branch의 High-level Feature Extractor는 서로 가중치를 공유함 
### (5) Fully Connected Block and Concatenation Block 
- FC Layer를 거쳐 분류 수행 
### (6) The Prior Distribution Label Smoothing (PDLS) Loss Function 
- Cross-entropy 대신 Label Smoothing (LS) Loss Function을 사용함 
- One-hot Encoding 벡터에서 0인 값은 모두 0보다 큰 값으로, 정답을 나타내는 1은 1보다 작은 값으로 Smoothing함 
- [0, 1, 0, 0, 0] → [0.02, 0.92, 0.02, 0.02, 0.02]
- Smoothing된 레이블을 기존 Hard 레이블 (0 또는 1로만 구성)을 대체하여 사용 
- 모델은 어떻게 성능을 높일지를 학습함과 동시에 Incorrect한 경우를 줄일 수 있을지를 학습함 
- 모델이 정답 레이블에 과잉 확신하는 현상을 해소 (보다 균일한 학습 가능) 
- 또한, 일반적인 LS는 모든 클래스에 대해 Uniform한 분포를 생성하지만, 우리는 표정 클래스에 대한 사전 지식이 있음 (Fear 클래스는 Disgust 클래스보다 Surprise 클래스와 더욱 혼동됨) 
- 데이터셋에서 제공되는 Label Distribution을 참고하여 Prior Distribution Label Smoothing을 고안함 

## 실험결과
- FER+, RAF-DB, AffectNet 데이터셋을 바탕으로 평가함 

## 의견
- Low-level Feature Extractor의 가중치 공유가 불가능하면 파라미터 수가 너무 많음 
- Super Resolution의 역할이 너무 작은 것 같음 
- Label Distribution이 없으면 PDLS 적용 불가? 
- 여러 스케일 정보를 획득하는 Pyramid 구조는 참고할 만함 

# An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ICLR, 2021)

[논문 링크](https://arxiv.org/abs/2010.11929)

<p align="center">
    <img width="600" alt='fig1' src="./img/01_16_01.png?raw=true"></br>
    <em><font size=2>Overall Architecture</font></em>
</p>

## 연구목적
- NLP에서 좋은 성능을 보여준 Transformer 구조는 뛰어난 효율성과 확장성 덕분에 모델 크기나 데이터셋 크기를 매우 키워도 포화될 조짐이 보이지 않음 
- 하지만 Computer Vision 분야는 여전히 Convolution 구조에 집착하고 있고, 점차 Convolution을 다른 기능을 하는 모듈로 변경하려는 움직임이 보이고 있음 
- NLP에서의 Transformer의 성공을 바탕으로, 본 연구에서는 Transformer 구조를 Image Recognition Task에 사용하는 방법을 제안하였고, 제안된 Vision Transformer (ViT)는 대용량의 데이터로 사전 훈련 시 많은 Task에서 SOTA 성능을 보임 

## 접근법
### (1) Vision Transformer (ViT) 
- 입력 이미지를 여러 개의 토큰으로 나누고 각 패치를 1차원 벡터로 Flatten함 
- 패치의 위치 정보 보존을 위해 Positional Embedding 수행 
- NLP에서의 Transformer Encoder와 마찬가지로 Multi-head Self-attention 및 MLP를 거침 
### (2) Fine-tuning and Higher Resolution 
- 매우 많은 데이터로 사전 학습시킨 후, 여러 Downstream Tasks로 Transfer Learning 시킴 

## 실험결과
- 여러 Tasks에서 SOTA 성능을 기록 

## 의견
- Transformer는 CNN 고유의 Inductive Biases가 부족하기 때문에 불충분한 양의 데이터에 대해 훈련되었을 때 일반화 성능이 낮음 (ResNet 보다 몇 퍼센트 포인트 낮은 정확도) 
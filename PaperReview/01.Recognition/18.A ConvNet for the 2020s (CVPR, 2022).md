# A ConvNet for the 2020s (CVPR, 2022)

[논문 링크](https://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html)

<p align="center">
    <img width="400" alt='fig1' src="./img/01_18_01.png?raw=true"></br>
    <em><font size=2>Overall Architecture</font></em>
</p>

## 연구목적
- 2010대에는 deep learning, 특히 CNN의 기념비적인 발전이 있었으며 visual recognition 분야는 기존의 hand-crafted method에서 CNN 위주로 변화함
- CNN의 translation invariance하다는 inductive bias와, convolution 연산의 computation을 share할 수 있다는 특징은 CNN을 recognition task를 넘어, 오랫동안 vision tasks의 general purpose backbone으로 자리잡게 했음
- 2020년 이후에는 NLP에서 활발히 사용되었던 Transformer 구조를 vision에 적용한 ViT가 등장하였음
- ViT는 CNN과 달리 어떠한 image-specific한 inductive bias도 지니고 있지 않았으며, 대신 scaling 측면에 집중하여 더 큰 모델, 더 많은 데이터셋에 대해 recognition task에서 CNN을 능가했음
- 하지만, detection과 segmentation 같은 tasks는 기존 CNN의 'sliding-window' 방식에 의존하고 있었고, CNN의 inductive bias 없이 vanilla ViT 모델로는 이러한 generic한 vision tasks에 적용되기 어려웠음
- CNN의 장점을 ViT에 결합한 hybrid approach가 이러한 간극을 좁혀주었음
- 대표적으로 Swin Transformer가 CNN의 sliding-window 방식을 Transformer 구조에 접합시켜 여러 generic한 tasks에 적용되어 SOTA 성능을 기록했음
- 이러한 Swin Transformer와 같은 hybrid model의 등장과 성공은 convolution의 본질이 결코 퇴색되지 않았으며 오히려 중요하다는 것을 의미함
- 이러한 관점에서 많은 vision을 위한 Transformer의 발전 방향은 convolution의 개념을 다시 가져오는 것이었음
- 하지만 이러한 접목 방식은 많은 비용을 수반하는데, 이는 sliding window 기반의 self-attention은 계산비용이 크며, 최적화를 위해서는 정교한 디자인 과정이 필요하기 때문임
- 따라서, 굳이 Transformer를 사용하지 않고 CNN 자체를 사용하는 방법을 선택할 수 있지만, 여전히 Transformer가 선택되는 이유는 Transformer가 우수한 scalability 및 multi-head attention을 바탕으로 현재 많은 vision tasks에서 CNN을 능가하고 있기 때문임
- 본 논문에서는 CNN과 Transformer의 아키텍처 수준의 차이를 분석하고 ViT 등장 이전과 이후의 CNN 모델들의 성능 격차를 줄이는 것을 목표로 하며, pure한 CNN 자체의 한계도 테스트해보고자 함
- 이를 위해 기본 ResNet으로부터 시작하여 Swin Transformer와 같은 hierarchical transformer architecture와 비슷한 구성으로 CNN 아키텍처를 점점 현대화(modernize)함
- 이러한 과정에서 Swin Transformer를 능가하는 순수 convolution으로만 이루어진 pure CNN인 ConvNeXt를 도출함

## 접근법
- ResNet-50을 베이스라인 모델로 설정
- ResNet-50을 순차적으로 개선함 (modernize함)

### (1) Training Techniques
- 모델의 아키텍처 만큼이나 training procedure도 성능 향상에 중요함
- 따라서 DeiT와 Swin Transformer의 training 방법을 적용함
> - Epoch:90 -> 300로 변경
> - AdamW optimizer 사용
> - Mixup, Cutmix, Random augmentation, random erasing과 같은 data augmentation 방법 적용
> - stochastic depth, label smoothing과 같은 regularization 방법 적용
- 적용결과 ResNet-50은 기존의 76.1%에서 78.8%로 성능이 증가함
- CNN과 Transformer의 성능 격차는 이러한 training procedure의 차이에서 비롯된 것일 수 있음

### (2) Macro Design
- Swin Transformer의 전체적인 아키텍처 디자인으 분석하고 이를 CNN에 적용하고자 함
- 먼저 stage compute ration를 변경함
> - Swin Transformer는 hierarchical 구조를 4개의 block 단위로 구분함 (ResNet도 마찬가지)
> - Swin Transformer-tiny에서는 1:1:3:1 비율로 block이 실행되며 large 모델은 1:1:9:1의 비율임
> - 따라서 기존 ResNet-50에서 사용하던 (3,4,6,3)의 residual block 개수를 (3,3,9,3)으로 변경시킴
> - 그 결과 정확도가 78.8%에서 79.4%로 높아졌고 block개수가 많아진 만큼 연산량도 증가
- 입력 이미지의 초기 처리 단계(stem)를 변경함 
> - 원본 이미지는 중복 정보가 많기 때문에 CNN과 vision transformer 모두 입력 이미지를 초기에 적절한 feature map 사이즈로 크게 줄임
> - ResNet-50의 경우 stride=2의 7x7 conv.와 max pool을 적용하여 입력 이미지의 resolution을 4배 downsample함
> - Vision transformer는 이보다 공격적으로 이미지를 14x14 혹은 16x16의 patch단위로 나누며 Swin Transformer는 더 작은 단위인 4x4 path단위로 나눔
> - 따라서 기존 ResNet-50의 stem을 stride=4의 4x4 conv. layer로 변경함
> - 이는 Swin Transformer의 stem과 같은 방식으로 동작함
> - 그 결과 정확도는 79.4%에서 79.5%로 미세하게 증가하였으며 연산량은 미세하게 감소함

### (3) ResNeXt-ify
- RssNeXt의 아이디어 적용하여 연산량과 정확도 간의 더 좋은 trade-off를 이끌어내고자 함
- ResNeXt의 특징은 input channel을 32개의 group로 나누어 각자 연산을 한 후 다시 concatenate시키는 grouped convolution을 수행한다는 것
- 본 논문에서는 이러한 개념을 더욱 확장한 depthwise convolution을 적용하여 연산량을 대폭 줄임
- 확장된 의미에서 depthwise convolution은 channel-wise self-attention weighted sum 연산과 유사함
- Swin-T와 채널을 맞추기 위해 width를 64에서 96으로 증가시킴
- 결과적으로 80.5%의 정확도 달성

### (4) Inverted Bottleneck
- Transformer의 MLP block에서는 채널을 4배 늘린 후 다시 원래로 되돌림 (inverted bottleneck)
- 따라서, 마찬가지로 기존 구조에 inverted bottleneck을 적용하여 기존에는 {1x1 conv.으로 채널 축소(384->96) -> 축소된 채널에서 3x3 conv. 적용(96->96) -> 1x1 conv.로 다시 채널 복구(96->384)} 순으로 진행되었던 bottleneck을 {1x1 conv.으로 채널 증폭(96->384) -> 증폭된 채널에서 3x3 conv. 적용(384->384) -> 1x1 conv.로 다시 채널 복구(384->96)} 순으로 설계
- 정확도가 약 0.1% 포인트만큼 증가

### (5) Large Kernel Sizes
- Vision transformers 강점 중 하나는 non-local self-attention을 통해 각 레이어가 global receptive field를 가지도록 한 점
- 마찬가지로 receptive field를 확장하기 위해 CNN에서도 large kernel size가 사용되기도 했지만, VGG-Net부터 시작된 3x3 kernel size를 stack하는 방법이 주를 이루었음
- 따라서, 본 논문에서는 기존 3x3의 기본 kernel size 대신 최소 7x7의 large kernel에 대해 실험함
- 먼저, depthwise convolution layer의 순서를 변경
> - 위의 inverted bottlneck 구조에서 1x1(96->384) -> 3x3(384->384) -> 1x1(384->96)의 conv 순서를 3x3(96->96) -> 1x1(96->384) -> 1x1(384->96)으로 변경
> - Depthwise convolution이 self-attention 과정과 유사하다면 Transformer의 block이 self-attention 수행후 MLP block을 거치는 것처럼, bottleneck의 연산 순서를 바꾸는 것이 적합함
> - 그 결과 연산량은 감소하였지만 정확도도 조금 낮아짐(79.9%)
- 다음으로 kernel size를 증가시킴
> - 기존 depthwise convolution layer의 kernel size를 7x7로 변경
> - 정확도가 79.9%(3x3) 에서 80.6%(7x7)으로 향상

### (6) Micro Design
- activation function, normalizeation layer와 같은 micro design에 대한 변경 적용
- 먼저 activation function을 변경함
> - NLP와 vision architecture 사이에는 activation function에 차이가 있음
> - 수많은 activation function이 개발되었지만 vision에서는 여전히 ReLU가 단순함과 효율성 측면에서 각광받음
> - Transformer도 초기에는 ReLu를 사용하였지만 최근에는 ReLU의 smoother vaiant 버전인 GELU(Gaussian Error Linear Unit)가 BERT, GPT-2, ViT등 최근 연구에서 많이 사용됨
> - 정확도와 연산량은 변하지 않았지만 본 연구에서는 모든 activation function으로 GELU를 사용
- 또한 activation function의 수를 줄임
> - 일반적인 CNN은 conv -> normalization -> activation 이 공식화 됨
> - 반면 Transformer는 적은 activation function을 사용: self-attention부분에는 activation이 없고 MLP block에 단 한개의 activation function만 사용
> - 이러한 Transformer의 전략을 가져오기 위해 bottleneck block의 두 개의 1x1 convolution 사이에 하나의 GELU만을 사용
> - 정확도를 81.3%까지 향상
- 마찬가지로 normalization layer의 수를 줄임
> - Transformer에서는 normalization layer도 self-attention, MLP block 앞에서만 수행
> - 따라서, 한개의 Batch Normalization만을 bottleneck의 첫번째 1x1 conv. 앞쪽에 적용시킴
> - 그 결과 81.4%의 성능 달성
- 다음으로 normalization layer를 변경함
> - Batch Normalization은 잘 수렴되며 overfitting을 방지하는 차원에서 CNN에서 널리 쓰였음
> - 하지만 batch size에 따른 성능 변화의 편차가 심하다는 등 모델의 성능에 좋지않은 영향을 줄 수 있는 intricacies가 있음
> - 반면 Transformer에서는 Layer Normalization을 통해 높은 성능을 기록함
> - 따라서 기존의 Batch Normalization을 Layer Normalization으로 변경
> - 그 결과 81.5%의 성능 달성
- 다음으로 downsampling layer를 분리함
> - ResNet은 각 block의 시작에서 3x3 conv.의 stride를 2로 조절하여 내부적으로 downsampling 수행
> - 하지만 Swin Transformer의 경우 patch merging이라는 별도의 downsampling layer가 각 스테이지 사이에 존재함
> - 따라서, block 내부에서 resolution을 줄이는 것이 아니라 stride=2의 2x2 conv. layer를 downsampling을 위해 추가함
> - 또한, 추가로 인한 발산을 방지하기 위해 Layer Normalization을 추가함
> - 그 결과 82.0%의 성능 달성
- 최종적으로 변경된 모델은 Swin Transformer를 능가하는 pure한 CNN이며 이를 ConvNeXt 모델을 도출함
- ConvNeXt는 Swin Transformer와 거의 유사한 크기 및 연산량, 처리속도 등을 지니지만 shifted window attention과 같은 부가적인 모듈 없이 순수 convolution으로만 구성됨
- Swin Transformer-tiny, small, base, large(Swin-T,S,B,L)과 유사한 복잡도를 갖는 ConvNeXt-tiny, small, base, large (ConvNeXt-T,S,B,L)를 정의
- ConvNeXt-T,B는 각각 ResNet-50,200의 modernized 버전이라고 할 수 있음
- 또한 ConvNeXt의 확장성을 실험하기 위해 더욱 거대한 ConvNeXt-XL도 설계

## 실험결과
- ImageNet-1K에서 Swin Transformer와 직접적으로 비교하면 같은 조건하에 조금씩 ConvNeXt가 더 성능이 좋음
- ViT와 같이 Inductive bias가 없거나 작은 모델의 장점은 ImageNet-22K와 데이터를 많이 넣었을때 성능이 더 좋다는 것임
- 하지만 ImageNet-22K에서의 평가 결과로부터 ConvNeXt는 비슷한 복잡도의 Swin Transformer의 성능과 비슷하거나 더 좋았으며, 이로부터 잘 디자인 된 CNN은 큰 데이터셋에 대한 성능이 뒤쳐지지 않으며 오히려 좋다는 것을 증명
- 또한, ConvNeXt-XL 모델은 더 좋은 성능을 기록하였는데 이는 ConvNeXt가 Transformer 구조와 마찬가지로 scalable architecture를 지닌다는 것을 증명함
- 또한, ConvNeXt는 object detection 및 segmentation tasks에서 Swin Transformer의 성능과 비슷하거나 더 좋았으며 이는 ConvNeXt가 general purpose backbone으로서의 역할을 할 수 있다는 것을 의미함

## 의견
- /